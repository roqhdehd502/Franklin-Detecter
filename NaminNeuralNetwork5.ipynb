{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Namin Neural Network.mk3\\n    -----------------\\n    conv1 - relu1 - pool1 - bn1\\n    conv2 - relu2 - pool2 - bn2\\n    conv3 - relu3 - pool3 - bn3\\n    affine - softmax\\n\\n    Parameters\\n    ----------\\n    Input Layer(Input Size): 100*100*3\\n    First Layer: Conv1(3EA, 3*3*3, Strides=3, Padding=1) - ReLU1\\n                - 34*34*3 - Pool1(2*2, Strides=1) - 33*33*3 - Bn1\\n    Second Layer: Conv2(6EA, 6*6*3, Strides=1, Padding=VALID) - ReLU2\\n                - 28*28*6 - Pool2(2*2, Strides=2) - 14*14*6 - Bn2\\n    Third Layer: Conv3(9EA, 3*3*6, Strides=3, Padding=2) - ReLU3\\n                - 6*6*9 - Pool3(2*2, Strides=2) - 3*3*9 - Bn3\\n    Output Layer: Affine(W=3*3*9, B=9) - Output Nodes = 3(Frank, Mike, T)\\n '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "    Namin Neural Network.mk3\n",
    "    -----------------\n",
    "    conv1 - relu1 - pool1 - bn1\n",
    "    conv2 - relu2 - pool2 - bn2\n",
    "    conv3 - relu3 - pool3 - bn3\n",
    "    affine - softmax\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Input Layer(Input Size): 100*100*3\n",
    "    First Layer: Conv1(3EA, 3*3*3, Strides=3, Padding=1) - ReLU1\n",
    "                - 34*34*3 - Pool1(2*2, Strides=1) - 33*33*3 - Bn1\n",
    "    Second Layer: Conv2(6EA, 6*6*3, Strides=1, Padding=VALID) - ReLU2\n",
    "                - 28*28*6 - Pool2(2*2, Strides=2) - 14*14*6 - Bn2\n",
    "    Third Layer: Conv3(9EA, 3*3*6, Strides=3, Padding=2) - ReLU3\n",
    "                - 6*6*9 - Pool3(2*2, Strides=2) - 3*3*9 - Bn3\n",
    "    Output Layer: Affine(W=3*3*9, B=9) - Output Nodes = 3\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to Images\n",
    "trainlist, testlist = [], []\n",
    "with open('train.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        trainlist.append([tmp[0], tmp[1]])\n",
    "        \n",
    "with open('test.txt') as f:\n",
    "    for line in f:\n",
    "        tmp = line.strip().split()\n",
    "        testlist.append([tmp[0], tmp[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "IMG_H = 100\n",
    "IMG_W = 100\n",
    "IMG_C = 3\n",
    "\n",
    "def readimg(path):\n",
    "    img = plt.imread(path)\n",
    "    return img\n",
    "\n",
    "def batch(path, batch_size):\n",
    "    img, label, paths = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        img.append(readimg(path[0][0]))\n",
    "        label.append(int(path[0][1]))\n",
    "        path.append(path.pop(0))\n",
    "        \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-5a87ce5d1f4d>:10: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From D:\\Others\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-5a87ce5d1f4d>:11: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-5a87ce5d1f4d>:12: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-5a87ce5d1f4d>:22: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-5a87ce5d1f4d>:24: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "num_class = 3\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    X = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, IMG_C]) # Input Layer\n",
    "    Y = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    with tf.variable_scope('CNN'):\n",
    "        # 1st Layer(Conv1 - relu1 - maxpool1 - bn1) = 33*33*3\n",
    "        conv1 = tf.layers.conv2d(X, 3, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, 2, (1, 1), padding='VALID')\n",
    "        bn1 = tf.compat.v1.layers.batch_normalization(pool1, training=True)\n",
    "        # 2nd Layer(Conv2 - relu2 - maxpool2 - bn2) = 14*14*6\n",
    "        conv2 = tf.layers.conv2d(bn1, 6, 6, (1, 1), padding='VALID', activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(conv2, 2, (2, 2), padding='VALID')\n",
    "        bn2 = tf.compat.v1.layers.batch_normalization(pool2, training=True)\n",
    "        # 3rd Layer(Conv3 - relu3 - maxpool3 - bn3) = 3*3*9\n",
    "        conv3 = tf.layers.conv2d(bn2, 9, 3, (3, 3), padding='SAME', activation=tf.nn.relu)\n",
    "        pool3 = tf.layers.max_pooling2d(conv3, 2, (2, 2), padding='VALID')\n",
    "        bn3 = tf.compat.v1.layers.batch_normalization(pool3, training=True)\n",
    "        # Fully Connected Layer(Affine)\n",
    "        affine1 = tf.layers.flatten(bn3)\n",
    "        # Output Layer\n",
    "        output = tf.layers.dense(affine1, num_class)\n",
    "        \n",
    "    # Softmax with Loss\n",
    "    with tf.variable_scope('Loss'):\n",
    "        Loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= Y, logits=output))\n",
    "    \n",
    "    # Training with Adam    \n",
    "    train_step = tf.train.AdamOptimizer(0.005).minimize(Loss) \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    tf.summary.scalar('Epoch-Loss', Loss)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size\n",
    "np.sum([np.product(var.shape) for var in g.get_collection('trainable_variables')]).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 1.3819227\n",
      "Epoch: 2 Loss: 0.35917968\n",
      "Epoch: 3 Loss: 0.19393207\n",
      "Epoch: 4 Loss: 0.16285183\n",
      "Epoch: 5 Loss: 0.14350526\n",
      "Epoch: 6 Loss: 0.18016289\n",
      "Epoch: 7 Loss: 0.096072815\n",
      "Epoch: 8 Loss: 0.17399748\n",
      "Epoch: 9 Loss: 0.15621638\n",
      "Epoch: 10 Loss: 0.07951683\n",
      "Epoch: 11 Loss: 0.0698328\n",
      "Epoch: 12 Loss: 0.06981332\n",
      "Epoch: 13 Loss: 0.25586703\n",
      "Epoch: 14 Loss: 0.1385676\n",
      "Epoch: 15 Loss: 0.067475356\n",
      "Epoch: 16 Loss: 0.041492168\n",
      "Epoch: 17 Loss: 0.042621274\n",
      "Epoch: 18 Loss: 0.059281852\n",
      "Epoch: 19 Loss: 0.05858392\n",
      "Epoch: 20 Loss: 0.02995291\n",
      "Epoch: 21 Loss: 0.025730137\n",
      "Epoch: 22 Loss: 0.020672137\n",
      "Epoch: 23 Loss: 0.044005826\n",
      "Epoch: 24 Loss: 0.044503685\n",
      "Epoch: 25 Loss: 0.026002387\n",
      "Epoch: 26 Loss: 0.020278605\n",
      "Epoch: 27 Loss: 0.012168524\n",
      "Epoch: 28 Loss: 0.01317683\n",
      "Epoch: 29 Loss: 0.014116745\n",
      "Epoch: 30 Loss: 0.015585636\n",
      "Epoch: 31 Loss: 0.019042492\n",
      "Epoch: 32 Loss: 0.01270023\n",
      "Epoch: 33 Loss: 0.009202935\n",
      "Epoch: 34 Loss: 0.007416865\n",
      "Epoch: 35 Loss: 0.0077478164\n",
      "Epoch: 36 Loss: 0.0067612333\n",
      "Epoch: 37 Loss: 0.00482946\n",
      "Epoch: 38 Loss: 0.008660909\n",
      "Epoch: 39 Loss: 0.008351675\n",
      "Epoch: 40 Loss: 0.006971903\n",
      "Epoch: 41 Loss: 0.0065188655\n",
      "Epoch: 42 Loss: 0.0042001493\n",
      "Epoch: 43 Loss: 0.0056892326\n",
      "Epoch: 44 Loss: 0.004416931\n",
      "Epoch: 45 Loss: 0.0069080982\n",
      "Epoch: 46 Loss: 0.006694206\n",
      "Epoch: 47 Loss: 0.0052296757\n",
      "Epoch: 48 Loss: 0.0051429337\n",
      "Epoch: 49 Loss: 0.0035536687\n",
      "Epoch: 50 Loss: 0.007294537\n",
      "Epoch: 51 Loss: 0.006013505\n",
      "WARNING:tensorflow:From D:\\Others\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 52 Loss: 0.0035404179\n",
      "Epoch: 53 Loss: 0.00448569\n",
      "Epoch: 54 Loss: 0.0036098438\n",
      "Epoch: 55 Loss: 0.0038688148\n",
      "Epoch: 56 Loss: 0.0035580972\n",
      "Epoch: 57 Loss: 0.007648157\n",
      "Epoch: 58 Loss: 0.0042297877\n",
      "Epoch: 59 Loss: 0.002386563\n",
      "Epoch: 60 Loss: 0.0038503788\n",
      "Epoch: 61 Loss: 0.0035456175\n",
      "Epoch: 62 Loss: 0.003308004\n",
      "Epoch: 63 Loss: 0.0031705\n",
      "Epoch: 64 Loss: 0.002940948\n",
      "Epoch: 65 Loss: 0.0034069887\n",
      "Epoch: 66 Loss: 0.0023008767\n",
      "Epoch: 67 Loss: 0.0036509912\n",
      "Epoch: 68 Loss: 0.0037170267\n",
      "Epoch: 69 Loss: 0.0027321598\n",
      "Epoch: 70 Loss: 0.002732028\n",
      "Epoch: 71 Loss: 0.0018494208\n",
      "Epoch: 72 Loss: 0.0032997367\n",
      "Epoch: 73 Loss: 0.0029435195\n",
      "Epoch: 74 Loss: 0.0029045555\n",
      "Epoch: 75 Loss: 0.0032987813\n",
      "Epoch: 76 Loss: 0.0023387116\n",
      "Epoch: 77 Loss: 0.0023678588\n",
      "Epoch: 78 Loss: 0.0019911244\n",
      "Epoch: 79 Loss: 0.003464303\n",
      "Epoch: 80 Loss: 0.0023914713\n",
      "Epoch: 81 Loss: 0.0016322762\n",
      "Epoch: 82 Loss: 0.0030173662\n",
      "Epoch: 83 Loss: 0.0024943491\n",
      "Epoch: 84 Loss: 0.0021475654\n",
      "Epoch: 85 Loss: 0.0020355247\n",
      "Epoch: 86 Loss: 0.0020907721\n",
      "Epoch: 87 Loss: 0.0021593878\n",
      "Epoch: 88 Loss: 0.0015345943\n",
      "Epoch: 89 Loss: 0.002845125\n",
      "Epoch: 90 Loss: 0.0028638653\n",
      "Epoch: 91 Loss: 0.0019593136\n",
      "Epoch: 92 Loss: 0.0018808536\n",
      "Epoch: 93 Loss: 0.0013093877\n",
      "Epoch: 94 Loss: 0.0022886111\n",
      "Epoch: 95 Loss: 0.0020613188\n",
      "Epoch: 96 Loss: 0.00252894\n",
      "Epoch: 97 Loss: 0.0025178443\n",
      "Epoch: 98 Loss: 0.0016883675\n",
      "Epoch: 99 Loss: 0.001720053\n",
      "Epoch: 100 Loss: 0.0013303208\n",
      "Epoch: 101 Loss: 0.0024784887\n",
      "Epoch: 102 Loss: 0.0019075286\n",
      "Epoch: 103 Loss: 0.0013420626\n",
      "Epoch: 104 Loss: 0.0022887636\n",
      "Epoch: 105 Loss: 0.0017316792\n",
      "Epoch: 106 Loss: 0.0016206666\n",
      "Epoch: 107 Loss: 0.0015438149\n",
      "Epoch: 108 Loss: 0.0019538992\n",
      "Epoch: 109 Loss: 0.0016892556\n",
      "Epoch: 110 Loss: 0.0011356227\n",
      "Epoch: 111 Loss: 0.0021241475\n",
      "Epoch: 112 Loss: 0.0020562168\n",
      "Epoch: 113 Loss: 0.0015345805\n",
      "Epoch: 114 Loss: 0.0014535637\n",
      "Epoch: 115 Loss: 0.0011040885\n",
      "Epoch: 116 Loss: 0.0017877091\n",
      "Epoch: 117 Loss: 0.0014213406\n",
      "Epoch: 118 Loss: 0.001945268\n",
      "Epoch: 119 Loss: 0.0019118668\n",
      "Epoch: 120 Loss: 0.001290869\n",
      "Epoch: 121 Loss: 0.0013468373\n",
      "Epoch: 122 Loss: 0.00097153074\n",
      "Epoch: 123 Loss: 0.0019512774\n",
      "Epoch: 124 Loss: 0.0016504346\n",
      "Epoch: 125 Loss: 0.0011772733\n",
      "Epoch: 126 Loss: 0.0017757346\n",
      "Epoch: 127 Loss: 0.0012643902\n",
      "Epoch: 128 Loss: 0.0012865547\n",
      "Epoch: 129 Loss: 0.001219882\n",
      "Epoch: 130 Loss: 0.00196931\n",
      "Epoch: 131 Loss: 0.0013660968\n",
      "Epoch: 132 Loss: 0.0008843151\n",
      "Epoch: 133 Loss: 0.0016774698\n",
      "Epoch: 134 Loss: 0.0014951708\n",
      "Epoch: 135 Loss: 0.0012313939\n",
      "Epoch: 136 Loss: 0.0011698075\n",
      "Epoch: 137 Loss: 0.000968222\n",
      "Epoch: 138 Loss: 0.0014118479\n",
      "Epoch: 139 Loss: 0.0010268352\n",
      "Epoch: 140 Loss: 0.0015956844\n",
      "Epoch: 141 Loss: 0.0015646865\n",
      "Epoch: 142 Loss: 0.0010551765\n",
      "Epoch: 143 Loss: 0.0010856618\n",
      "Epoch: 144 Loss: 0.00074867904\n",
      "Epoch: 145 Loss: 0.001511143\n",
      "Epoch: 146 Loss: 0.0013565267\n",
      "Epoch: 147 Loss: 0.0011320397\n",
      "Epoch: 148 Loss: 0.0014513694\n",
      "Epoch: 149 Loss: 0.0009862854\n",
      "Epoch: 150 Loss: 0.0010325952\n",
      "Epoch: 151 Loss: 0.0009200058\n",
      "Epoch: 152 Loss: 0.0016872832\n",
      "Epoch: 153 Loss: 0.0011232421\n",
      "Epoch: 154 Loss: 0.0007175713\n",
      "Epoch: 155 Loss: 0.0013703771\n",
      "Epoch: 156 Loss: 0.0011306435\n",
      "Epoch: 157 Loss: 0.00097602763\n",
      "Epoch: 158 Loss: 0.00094809604\n",
      "Epoch: 159 Loss: 0.0009186157\n",
      "Epoch: 160 Loss: 0.0011201522\n",
      "Epoch: 161 Loss: 0.0007597216\n",
      "Epoch: 162 Loss: 0.001343126\n",
      "Epoch: 163 Loss: 0.0013163987\n",
      "Epoch: 164 Loss: 0.00090332085\n",
      "Epoch: 165 Loss: 0.0008979921\n",
      "Epoch: 166 Loss: 0.0006206751\n",
      "Epoch: 167 Loss: 0.0011902432\n",
      "Epoch: 168 Loss: 0.0011543804\n",
      "Epoch: 169 Loss: 0.0011830305\n",
      "Epoch: 170 Loss: 0.0012165175\n",
      "Epoch: 171 Loss: 0.000796421\n",
      "Epoch: 172 Loss: 0.0008418148\n",
      "Epoch: 173 Loss: 0.00068589544\n",
      "Epoch: 174 Loss: 0.001359446\n",
      "Epoch: 175 Loss: 0.0009739501\n",
      "Epoch: 176 Loss: 0.0006238835\n",
      "Epoch: 177 Loss: 0.0011426561\n",
      "Epoch: 178 Loss: 0.00088923814\n",
      "Epoch: 179 Loss: 0.0008153678\n",
      "Epoch: 180 Loss: 0.0007758408\n",
      "Epoch: 181 Loss: 0.00093365414\n",
      "Epoch: 182 Loss: 0.00091356505\n",
      "Epoch: 183 Loss: 0.0006047702\n",
      "Epoch: 184 Loss: 0.0011089467\n",
      "Epoch: 185 Loss: 0.0011063949\n",
      "Epoch: 186 Loss: 0.00078288873\n",
      "Epoch: 187 Loss: 0.0007433114\n",
      "Epoch: 188 Loss: 0.00055256044\n",
      "Epoch: 189 Loss: 0.001000278\n",
      "Epoch: 190 Loss: 0.00086865434\n",
      "Epoch: 191 Loss: 0.0010283303\n",
      "Epoch: 192 Loss: 0.0010172873\n",
      "Epoch: 193 Loss: 0.00066279294\n",
      "Epoch: 194 Loss: 0.0007058746\n",
      "Epoch: 195 Loss: 0.0005284496\n",
      "Epoch: 196 Loss: 0.0010896614\n",
      "Epoch: 197 Loss: 0.0008927216\n",
      "Epoch: 198 Loss: 0.0005803303\n",
      "Epoch: 199 Loss: 0.0009617276\n",
      "Epoch: 200 Loss: 0.0006884414\n",
      "Epoch: 201 Loss: 0.000688425\n",
      "Epoch: 202 Loss: 0.0006512746\n",
      "Epoch: 203 Loss: 0.0009898526\n",
      "Epoch: 204 Loss: 0.0007751019\n",
      "Epoch: 205 Loss: 0.00048663138\n",
      "Epoch: 206 Loss: 0.0009325826\n",
      "Epoch: 207 Loss: 0.0008562385\n",
      "Epoch: 208 Loss: 0.0006761542\n",
      "Epoch: 209 Loss: 0.00063555135\n",
      "Epoch: 210 Loss: 0.00051270355\n",
      "Epoch: 211 Loss: 0.00083756633\n",
      "Epoch: 212 Loss: 0.0006273835\n",
      "Epoch: 213 Loss: 0.0008934374\n",
      "Epoch: 214 Loss: 0.0008663433\n",
      "Epoch: 215 Loss: 0.0005730727\n",
      "Epoch: 216 Loss: 0.0006062175\n",
      "Epoch: 217 Loss: 0.00042551183\n",
      "Epoch: 218 Loss: 0.00090698304\n",
      "Epoch: 219 Loss: 0.0007916072\n",
      "Epoch: 220 Loss: 0.0005757579\n",
      "Epoch: 221 Loss: 0.0008296882\n",
      "Epoch: 222 Loss: 0.0005582366\n",
      "Epoch: 223 Loss: 0.0005966651\n",
      "Epoch: 224 Loss: 0.0005600017\n",
      "Epoch: 225 Loss: 0.0010460216\n",
      "Epoch: 226 Loss: 0.000663707\n",
      "Epoch: 227 Loss: 0.00040964584\n",
      "Epoch: 228 Loss: 0.0008025439\n",
      "Epoch: 229 Loss: 0.000677588\n",
      "Epoch: 230 Loss: 0.00057776086\n",
      "Epoch: 231 Loss: 0.00055319414\n",
      "Epoch: 232 Loss: 0.0004932353\n",
      "Epoch: 233 Loss: 0.00069572945\n",
      "Epoch: 234 Loss: 0.00047626943\n",
      "Epoch: 235 Loss: 0.00077719445\n",
      "Epoch: 236 Loss: 0.00075983576\n",
      "Epoch: 237 Loss: 0.0005120542\n",
      "Epoch: 238 Loss: 0.0005272751\n",
      "Epoch: 239 Loss: 0.0003639564\n",
      "Epoch: 240 Loss: 0.00075609726\n",
      "Epoch: 241 Loss: 0.0007017953\n",
      "Epoch: 242 Loss: 0.0006113623\n",
      "Epoch: 243 Loss: 0.00072219473\n",
      "Epoch: 244 Loss: 0.0004702571\n",
      "Epoch: 245 Loss: 0.00050914777\n",
      "Epoch: 246 Loss: 0.0004340377\n",
      "Epoch: 247 Loss: 0.0008709718\n",
      "Epoch: 248 Loss: 0.00059250527\n",
      "Epoch: 249 Loss: 0.00036328938\n",
      "Epoch: 250 Loss: 0.00069497485\n",
      "Epoch: 251 Loss: 0.0005536862\n",
      "Epoch: 252 Loss: 0.00049482664\n",
      "Epoch: 253 Loss: 0.0004749646\n",
      "Epoch: 254 Loss: 0.00051924534\n",
      "Epoch: 255 Loss: 0.000579061\n",
      "Epoch: 256 Loss: 0.00038652917\n",
      "Epoch: 257 Loss: 0.00067612255\n",
      "Epoch: 258 Loss: 0.00066770473\n",
      "Epoch: 259 Loss: 0.00046884446\n",
      "Epoch: 260 Loss: 0.00046191065\n",
      "Epoch: 261 Loss: 0.00032935597\n",
      "Epoch: 262 Loss: 0.0006409435\n",
      "Epoch: 263 Loss: 0.0006060049\n",
      "Epoch: 264 Loss: 0.0006348844\n",
      "Epoch: 265 Loss: 0.00062893913\n",
      "Epoch: 266 Loss: 0.00040310214\n",
      "Epoch: 267 Loss: 0.00044522382\n",
      "Epoch: 268 Loss: 0.00034861575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 Loss: 0.00071099296\n",
      "Epoch: 270 Loss: 0.00054122467\n",
      "Epoch: 271 Loss: 0.0003373319\n",
      "Epoch: 272 Loss: 0.0006072743\n",
      "Epoch: 273 Loss: 0.00044426555\n",
      "Epoch: 274 Loss: 0.00043964727\n",
      "Epoch: 275 Loss: 0.00041954627\n",
      "Epoch: 276 Loss: 0.0005484054\n",
      "Epoch: 277 Loss: 0.0005001764\n",
      "Epoch: 278 Loss: 0.0003150458\n",
      "Epoch: 279 Loss: 0.0005893107\n",
      "Epoch: 280 Loss: 0.0005729623\n",
      "Epoch: 281 Loss: 0.00043880485\n",
      "Epoch: 282 Loss: 0.0004076848\n",
      "Epoch: 283 Loss: 0.00031195738\n",
      "Epoch: 284 Loss: 0.00054916536\n",
      "Epoch: 285 Loss: 0.0004430061\n",
      "Epoch: 286 Loss: 0.00056389795\n",
      "Epoch: 287 Loss: 0.0005547115\n",
      "Epoch: 288 Loss: 0.00036078697\n",
      "Epoch: 289 Loss: 0.00039320474\n",
      "Epoch: 290 Loss: 0.00028419553\n",
      "Epoch: 291 Loss: 0.0006045909\n",
      "Epoch: 292 Loss: 0.0005103713\n",
      "Epoch: 293 Loss: 0.00033674433\n",
      "Epoch: 294 Loss: 0.00053149316\n",
      "Epoch: 295 Loss: 0.0003673604\n",
      "Epoch: 296 Loss: 0.000392704\n",
      "Epoch: 297 Loss: 0.00036859038\n",
      "Epoch: 298 Loss: 0.0006115126\n",
      "Epoch: 299 Loss: 0.00043885424\n",
      "Epoch: 300 Loss: 0.0002697489\n",
      "Epoch: 301 Loss: 0.00052070105\n",
      "Epoch: 302 Loss: 0.0004611731\n",
      "Epoch: 303 Loss: 0.0003909906\n",
      "Epoch: 304 Loss: 0.00036249112\n",
      "Epoch: 305 Loss: 0.00030596074\n",
      "Epoch: 306 Loss: 0.0004789818\n",
      "Epoch: 307 Loss: 0.0003386095\n",
      "Epoch: 308 Loss: 0.00051259826\n",
      "Epoch: 309 Loss: 0.00049408886\n",
      "Epoch: 310 Loss: 0.0003323191\n",
      "Epoch: 311 Loss: 0.00035122698\n",
      "Epoch: 312 Loss: 0.00024335532\n",
      "Epoch: 313 Loss: 0.00052024965\n",
      "Epoch: 314 Loss: 0.00047561357\n",
      "Epoch: 315 Loss: 0.00036030076\n",
      "Epoch: 316 Loss: 0.00047889235\n",
      "Epoch: 317 Loss: 0.0003139706\n",
      "Epoch: 318 Loss: 0.0003504228\n",
      "Epoch: 319 Loss: 0.00031297002\n",
      "Epoch: 320 Loss: 0.0006084542\n",
      "Epoch: 321 Loss: 0.00039787576\n",
      "Epoch: 322 Loss: 0.00023795423\n",
      "Epoch: 323 Loss: 0.0004663235\n",
      "Epoch: 324 Loss: 0.00038877316\n",
      "Epoch: 325 Loss: 0.00034259623\n",
      "Epoch: 326 Loss: 0.00032517672\n",
      "Epoch: 327 Loss: 0.00031631865\n",
      "Epoch: 328 Loss: 0.0004068864\n",
      "Epoch: 329 Loss: 0.00027021003\n",
      "Epoch: 330 Loss: 0.00046549726\n",
      "Epoch: 331 Loss: 0.00045024857\n",
      "Epoch: 332 Loss: 0.0003115367\n",
      "Epoch: 333 Loss: 0.00031548087\n",
      "Epoch: 334 Loss: 0.00021908384\n",
      "Epoch: 335 Loss: 0.00045037505\n",
      "Epoch: 336 Loss: 0.000441289\n",
      "Epoch: 337 Loss: 0.00040508556\n",
      "Epoch: 338 Loss: 0.0004314444\n",
      "Epoch: 339 Loss: 0.00027448678\n",
      "Epoch: 340 Loss: 0.00030895913\n",
      "Epoch: 341 Loss: 0.0002521593\n",
      "Epoch: 342 Loss: 0.000525083\n",
      "Epoch: 343 Loss: 0.00037089185\n",
      "Epoch: 344 Loss: 0.00022204871\n",
      "Epoch: 345 Loss: 0.00042033088\n",
      "Epoch: 346 Loss: 0.00032153353\n",
      "Epoch: 347 Loss: 0.00030511516\n",
      "Epoch: 348 Loss: 0.0002885659\n",
      "Epoch: 349 Loss: 0.00034676862\n",
      "Epoch: 350 Loss: 0.00035858375\n",
      "Epoch: 351 Loss: 0.0002276576\n",
      "Epoch: 352 Loss: 0.00041894492\n",
      "Epoch: 353 Loss: 0.00041158014\n",
      "Epoch: 354 Loss: 0.000298374\n",
      "Epoch: 355 Loss: 0.00028395743\n",
      "Epoch: 356 Loss: 0.00020845483\n",
      "Epoch: 357 Loss: 0.00039896654\n",
      "Epoch: 358 Loss: 0.00035916804\n",
      "Epoch: 359 Loss: 0.0004002889\n",
      "Epoch: 360 Loss: 0.00039061287\n",
      "Epoch: 361 Loss: 0.000248753\n",
      "Epoch: 362 Loss: 0.00027657786\n",
      "Epoch: 363 Loss: 0.00020785707\n",
      "Epoch: 364 Loss: 0.00044533372\n",
      "Epoch: 365 Loss: 0.00036444634\n",
      "Epoch: 366 Loss: 0.00021991167\n",
      "Epoch: 367 Loss: 0.00038136082\n",
      "Epoch: 368 Loss: 0.00026840615\n",
      "Epoch: 369 Loss: 0.0002768667\n",
      "Epoch: 370 Loss: 0.00026173925\n",
      "Epoch: 371 Loss: 0.00038875054\n",
      "Epoch: 372 Loss: 0.00032078667\n",
      "Epoch: 373 Loss: 0.00019473805\n",
      "Epoch: 374 Loss: 0.00037854433\n",
      "Epoch: 375 Loss: 0.00035138012\n",
      "Epoch: 376 Loss: 0.00028105572\n",
      "Epoch: 377 Loss: 0.00025831742\n",
      "Epoch: 378 Loss: 0.00020639287\n",
      "Epoch: 379 Loss: 0.00035446245\n",
      "Epoch: 380 Loss: 0.00027103716\n",
      "Epoch: 381 Loss: 0.00036882813\n",
      "Epoch: 382 Loss: 0.00035735947\n",
      "Epoch: 383 Loss: 0.00022717923\n",
      "Epoch: 384 Loss: 0.00025045287\n",
      "Epoch: 385 Loss: 0.00017601205\n",
      "Epoch: 386 Loss: 0.00039242508\n",
      "Epoch: 387 Loss: 0.0003451677\n",
      "Epoch: 388 Loss: 0.00023201571\n",
      "Epoch: 389 Loss: 0.00034900845\n",
      "Epoch: 390 Loss: 0.00022797102\n",
      "Epoch: 391 Loss: 0.0002527321\n",
      "Epoch: 392 Loss: 0.00023677203\n",
      "Epoch: 393 Loss: 0.0004527494\n",
      "Epoch: 394 Loss: 0.00029296972\n",
      "Epoch: 395 Loss: 0.0001735704\n",
      "Epoch: 396 Loss: 0.00034326696\n",
      "Epoch: 397 Loss: 0.00029088056\n",
      "Epoch: 398 Loss: 0.00025055662\n",
      "Epoch: 399 Loss: 0.0002360031\n",
      "Epoch: 400 Loss: 0.00021030754\n",
      "Epoch: 401 Loss: 0.000315954\n",
      "Epoch: 402 Loss: 0.00021450382\n",
      "Epoch: 403 Loss: 0.0003438229\n",
      "Epoch: 404 Loss: 0.00033129723\n",
      "Epoch: 405 Loss: 0.00021665801\n",
      "Epoch: 406 Loss: 0.00022937669\n",
      "Epoch: 407 Loss: 0.00015749843\n",
      "Epoch: 408 Loss: 0.00034312048\n",
      "Epoch: 409 Loss: 0.00032033474\n",
      "Epoch: 410 Loss: 0.00026176812\n",
      "Epoch: 411 Loss: 0.00032178464\n",
      "Epoch: 412 Loss: 0.00020065135\n",
      "Epoch: 413 Loss: 0.00022740773\n",
      "Epoch: 414 Loss: 0.00019589503\n",
      "Epoch: 415 Loss: 0.00040918696\n",
      "Epoch: 416 Loss: 0.00027220722\n",
      "Epoch: 417 Loss: 0.00015964705\n",
      "Epoch: 418 Loss: 0.00031450417\n",
      "Epoch: 419 Loss: 0.00024373386\n",
      "Epoch: 420 Loss: 0.00022386528\n",
      "Epoch: 421 Loss: 0.00021355406\n",
      "Epoch: 422 Loss: 0.00023105014\n",
      "Epoch: 423 Loss: 0.00027401937\n",
      "Epoch: 424 Loss: 0.0001798674\n",
      "Epoch: 425 Loss: 0.00031666554\n",
      "Epoch: 426 Loss: 0.00030887764\n",
      "Epoch: 427 Loss: 0.00021024127\n",
      "Epoch: 428 Loss: 0.00021005151\n",
      "Epoch: 429 Loss: 0.00014781539\n",
      "Epoch: 430 Loss: 0.0003061284\n",
      "Epoch: 431 Loss: 0.00030370886\n",
      "Epoch: 432 Loss: 0.0003035959\n",
      "Epoch: 433 Loss: 0.00029697825\n",
      "Epoch: 434 Loss: 0.00018064985\n",
      "Epoch: 435 Loss: 0.00020605951\n",
      "Epoch: 436 Loss: 0.00016133761\n",
      "Epoch: 437 Loss: 0.0003531917\n",
      "Epoch: 438 Loss: 0.00026418982\n",
      "Epoch: 439 Loss: 0.00015577256\n",
      "Epoch: 440 Loss: 0.00028969807\n",
      "Epoch: 441 Loss: 0.00020617344\n",
      "Epoch: 442 Loss: 0.00020559852\n",
      "Epoch: 443 Loss: 0.00019526869\n",
      "Epoch: 444 Loss: 0.0002609766\n",
      "Epoch: 445 Loss: 0.00024798358\n",
      "Epoch: 446 Loss: 0.0001525509\n",
      "Epoch: 447 Loss: 0.00028955904\n",
      "Epoch: 448 Loss: 0.00028723275\n",
      "Epoch: 449 Loss: 0.0002078804\n",
      "Epoch: 450 Loss: 0.00019294799\n",
      "Epoch: 451 Loss: 0.00014653975\n",
      "Epoch: 452 Loss: 0.00027585984\n",
      "Epoch: 453 Loss: 0.00022784589\n",
      "Epoch: 454 Loss: 0.00028069425\n",
      "Epoch: 455 Loss: 0.000272615\n",
      "Epoch: 456 Loss: 0.00016847036\n",
      "Epoch: 457 Loss: 0.00018871631\n",
      "Epoch: 458 Loss: 0.0001370234\n",
      "Epoch: 459 Loss: 0.000307685\n",
      "Epoch: 460 Loss: 0.00026126634\n",
      "Epoch: 461 Loss: 0.00016064289\n",
      "Epoch: 462 Loss: 0.00026719042\n",
      "Epoch: 463 Loss: 0.00017882789\n",
      "Epoch: 464 Loss: 0.00019152484\n",
      "Epoch: 465 Loss: 0.00017879272\n",
      "Epoch: 466 Loss: 0.00030346445\n",
      "Epoch: 467 Loss: 0.00022622594\n",
      "Epoch: 468 Loss: 0.00013446211\n",
      "Epoch: 469 Loss: 0.00026563354\n",
      "Epoch: 470 Loss: 0.00023467906\n",
      "Epoch: 471 Loss: 0.00019337228\n",
      "Epoch: 472 Loss: 0.00017852041\n",
      "Epoch: 473 Loss: 0.00015042523\n",
      "Epoch: 474 Loss: 0.00024896505\n",
      "Epoch: 475 Loss: 0.00017732677\n",
      "Epoch: 476 Loss: 0.0002636185\n",
      "Epoch: 477 Loss: 0.00025369303\n",
      "Epoch: 478 Loss: 0.00016001365\n",
      "Epoch: 479 Loss: 0.00017472488\n",
      "Epoch: 480 Loss: 0.00012061722\n",
      "Epoch: 481 Loss: 0.00027352406\n",
      "Epoch: 482 Loss: 0.00024771516\n",
      "Epoch: 483 Loss: 0.00017739926\n",
      "Epoch: 484 Loss: 0.00024827413\n",
      "Epoch: 485 Loss: 0.00015615018\n",
      "Epoch: 486 Loss: 0.00017734108\n",
      "Epoch: 487 Loss: 0.00016081597\n",
      "Epoch: 488 Loss: 0.00032954811\n",
      "Epoch: 489 Loss: 0.0002098611\n",
      "Epoch: 490 Loss: 0.00012133146\n",
      "Epoch: 491 Loss: 0.00024458556\n",
      "Epoch: 492 Loss: 0.00019558208\n",
      "Epoch: 493 Loss: 0.00017590287\n",
      "Epoch: 494 Loss: 0.00016670102\n",
      "Epoch: 495 Loss: 0.00015840137\n",
      "Epoch: 496 Loss: 0.00021938936\n",
      "Epoch: 497 Loss: 0.0001447682\n",
      "Epoch: 498 Loss: 0.00024743803\n",
      "Epoch: 499 Loss: 0.00023856286\n",
      "Epoch: 500 Loss: 0.00015655615\n",
      "Epoch: 501 Loss: 0.00016294279\n",
      "Epoch: 502 Loss: 0.000112516565\n",
      "Epoch: 503 Loss: 0.00024403301\n",
      "Epoch: 504 Loss: 0.00023483738\n",
      "Epoch: 505 Loss: 0.00020693839\n",
      "Epoch: 506 Loss: 0.00023098348\n",
      "Epoch: 507 Loss: 0.00013957274\n",
      "Epoch: 508 Loss: 0.00016241155\n",
      "Epoch: 509 Loss: 0.00013321106\n",
      "Epoch: 510 Loss: 0.00029065768\n",
      "Epoch: 511 Loss: 0.00020113046\n",
      "Epoch: 512 Loss: 0.00011544848\n",
      "Epoch: 513 Loss: 0.00022693208\n",
      "Epoch: 514 Loss: 0.00016777759\n",
      "Epoch: 515 Loss: 0.00016090917\n",
      "Epoch: 516 Loss: 0.00015200015\n",
      "Epoch: 517 Loss: 0.00018297916\n",
      "Epoch: 518 Loss: 0.00019776609\n",
      "Epoch: 519 Loss: 0.00012449682\n",
      "Epoch: 520 Loss: 0.00022930275\n",
      "Epoch: 521 Loss: 0.0002248119\n",
      "Epoch: 522 Loss: 0.00015640551\n",
      "Epoch: 523 Loss: 0.0001511905\n",
      "Epoch: 524 Loss: 0.00010961554\n",
      "Epoch: 525 Loss: 0.00022127535\n",
      "Epoch: 526 Loss: 0.00020399208\n",
      "Epoch: 527 Loss: 0.00022097692\n",
      "Epoch: 528 Loss: 0.00021551893\n",
      "Epoch: 529 Loss: 0.00013011617\n",
      "Epoch: 530 Loss: 0.0001490571\n",
      "Epoch: 531 Loss: 0.000112812755\n",
      "Epoch: 532 Loss: 0.00025253464\n",
      "Epoch: 533 Loss: 0.00020094938\n",
      "Epoch: 534 Loss: 0.00011665784\n",
      "Epoch: 535 Loss: 0.00021181964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 536 Loss: 0.00014602381\n",
      "Epoch: 537 Loss: 0.00014992495\n",
      "Epoch: 538 Loss: 0.00014200823\n",
      "Epoch: 539 Loss: 0.00021133042\n",
      "Epoch: 540 Loss: 0.00018186077\n",
      "Epoch: 541 Loss: 0.000108432374\n",
      "Epoch: 542 Loss: 0.00021230102\n",
      "Epoch: 543 Loss: 0.00020036132\n",
      "Epoch: 544 Loss: 0.000153047\n",
      "Epoch: 545 Loss: 0.00014080739\n",
      "Epoch: 546 Loss: 0.0001117994\n",
      "Epoch: 547 Loss: 0.00020278667\n",
      "Epoch: 548 Loss: 0.00015523385\n",
      "Epoch: 549 Loss: 0.00020575008\n",
      "Epoch: 550 Loss: 0.00020120613\n",
      "Epoch: 551 Loss: 0.0001242602\n",
      "Epoch: 552 Loss: 0.00013860155\n",
      "Epoch: 553 Loss: 9.746389e-05\n",
      "Epoch: 554 Loss: 0.00022391863\n",
      "Epoch: 555 Loss: 0.00019736166\n",
      "Epoch: 556 Loss: 0.00012559634\n",
      "Epoch: 557 Loss: 0.00019812903\n",
      "Epoch: 558 Loss: 0.00012662368\n",
      "Epoch: 559 Loss: 0.00014182196\n",
      "Epoch: 560 Loss: 0.00013212576\n",
      "Epoch: 561 Loss: 0.0002528096\n",
      "Epoch: 562 Loss: 0.00016742607\n",
      "Epoch: 563 Loss: 9.740551e-05\n",
      "Epoch: 564 Loss: 0.00019708219\n",
      "Epoch: 565 Loss: 0.0001667519\n",
      "Epoch: 566 Loss: 0.00014261686\n",
      "Epoch: 567 Loss: 0.00013207659\n",
      "Epoch: 568 Loss: 0.00011677429\n",
      "Epoch: 569 Loss: 0.00018273838\n",
      "Epoch: 570 Loss: 0.00012405055\n",
      "Epoch: 571 Loss: 0.00019579315\n",
      "Epoch: 572 Loss: 0.00019062798\n",
      "Epoch: 573 Loss: 0.000120257784\n",
      "Epoch: 574 Loss: 0.00013026629\n",
      "Epoch: 575 Loss: 8.915046e-05\n",
      "Epoch: 576 Loss: 0.0002017493\n",
      "Epoch: 577 Loss: 0.00018712935\n",
      "Epoch: 578 Loss: 0.00014340277\n",
      "Epoch: 579 Loss: 0.000186481\n",
      "Epoch: 580 Loss: 0.000112774454\n",
      "Epoch: 581 Loss: 0.00013169077\n",
      "Epoch: 582 Loss: 0.00011399322\n",
      "Epoch: 583 Loss: 0.0002463476\n",
      "Epoch: 584 Loss: 0.00015864175\n",
      "Epoch: 585 Loss: 9.0213915e-05\n",
      "Epoch: 586 Loss: 0.00018397039\n",
      "Epoch: 587 Loss: 0.00014085865\n",
      "Epoch: 588 Loss: 0.00013009943\n",
      "Epoch: 589 Loss: 0.00012369828\n",
      "Epoch: 590 Loss: 0.00013011167\n",
      "Epoch: 591 Loss: 0.0001631648\n",
      "Epoch: 592 Loss: 0.00010466292\n",
      "Epoch: 593 Loss: 0.00018449473\n",
      "Epoch: 594 Loss: 0.0001816442\n",
      "Epoch: 595 Loss: 0.000120816716\n",
      "Epoch: 596 Loss: 0.00012242835\n",
      "Epoch: 597 Loss: 8.521249e-05\n",
      "Epoch: 598 Loss: 0.00018175103\n",
      "Epoch: 599 Loss: 0.0001813073\n",
      "Epoch: 600 Loss: 0.00017309787\n",
      "Epoch: 601 Loss: 0.00017619546\n",
      "Epoch: 602 Loss: 0.00010390084\n",
      "Epoch: 603 Loss: 0.00012182934\n",
      "Epoch: 604 Loss: 9.573586e-05\n",
      "Epoch: 605 Loss: 0.0002163296\n",
      "Epoch: 606 Loss: 0.00015547406\n",
      "Epoch: 607 Loss: 8.9256515e-05\n",
      "Epoch: 608 Loss: 0.00017308345\n",
      "Epoch: 609 Loss: 0.00012396243\n",
      "Epoch: 610 Loss: 0.00012151763\n",
      "Epoch: 611 Loss: 0.000115066425\n",
      "Epoch: 612 Loss: 0.00014970219\n",
      "Epoch: 613 Loss: 0.00014911147\n",
      "Epoch: 614 Loss: 9.075461e-05\n",
      "Epoch: 615 Loss: 0.00017230373\n",
      "Epoch: 616 Loss: 0.00017271568\n",
      "Epoch: 617 Loss: 0.00012227077\n",
      "Epoch: 618 Loss: 0.00011440277\n",
      "Epoch: 619 Loss: 8.5361105e-05\n",
      "Epoch: 620 Loss: 0.00016743068\n",
      "Epoch: 621 Loss: 0.00014304662\n",
      "Epoch: 622 Loss: 0.00016718754\n",
      "Epoch: 623 Loss: 0.00016488519\n",
      "Epoch: 624 Loss: 9.88624e-05\n",
      "Epoch: 625 Loss: 0.000112920105\n",
      "Epoch: 626 Loss: 8.226773e-05\n",
      "Epoch: 627 Loss: 0.00018742989\n",
      "Epoch: 628 Loss: 0.00015925209\n",
      "Epoch: 629 Loss: 9.3325434e-05\n",
      "Epoch: 630 Loss: 0.00016257277\n",
      "Epoch: 631 Loss: 0.00010667376\n",
      "Epoch: 632 Loss: 0.000115049515\n",
      "Epoch: 633 Loss: 0.00010776548\n",
      "Epoch: 634 Loss: 0.00017989289\n",
      "Epoch: 635 Loss: 0.00013822601\n",
      "Epoch: 636 Loss: 8.055868e-05\n",
      "Epoch: 637 Loss: 0.00016082602\n",
      "Epoch: 638 Loss: 0.0001452783\n",
      "Epoch: 639 Loss: 0.00011757041\n",
      "Epoch: 640 Loss: 0.00010788795\n",
      "Epoch: 641 Loss: 8.913199e-05\n",
      "Epoch: 642 Loss: 0.00015384344\n",
      "Epoch: 643 Loss: 0.0001108186\n",
      "Epoch: 644 Loss: 0.0001598539\n",
      "Epoch: 645 Loss: 0.00015612938\n",
      "Epoch: 646 Loss: 9.533294e-05\n",
      "Epoch: 647 Loss: 0.00010646895\n",
      "Epoch: 648 Loss: 7.309054e-05\n",
      "Epoch: 649 Loss: 0.00017001989\n",
      "Epoch: 650 Loss: 0.0001535474\n",
      "Epoch: 651 Loss: 0.00010384129\n",
      "Epoch: 652 Loss: 0.00015378372\n",
      "Epoch: 653 Loss: 9.341731e-05\n",
      "Epoch: 654 Loss: 0.0001094471\n",
      "Epoch: 655 Loss: 0.00010044491\n",
      "Epoch: 656 Loss: 0.00021325421\n",
      "Epoch: 657 Loss: 0.00013015658\n",
      "Epoch: 658 Loss: 7.421017e-05\n",
      "Epoch: 659 Loss: 0.00015079715\n",
      "Epoch: 660 Loss: 0.00012223801\n",
      "Epoch: 661 Loss: 0.00010878732\n",
      "Epoch: 662 Loss: 0.00010255006\n",
      "Epoch: 663 Loss: 9.54674e-05\n",
      "Epoch: 664 Loss: 0.00013984212\n",
      "Epoch: 665 Loss: 9.118886e-05\n",
      "Epoch: 666 Loss: 0.00015297202\n",
      "Epoch: 667 Loss: 0.0001498146\n",
      "Epoch: 668 Loss: 9.519165e-05\n",
      "Epoch: 669 Loss: 0.00010081313\n",
      "Epoch: 670 Loss: 6.897545e-05\n",
      "Epoch: 671 Loss: 0.00015679524\n",
      "Epoch: 672 Loss: 0.00014853483\n",
      "Epoch: 673 Loss: 0.00012326255\n",
      "Epoch: 674 Loss: 0.00014602047\n",
      "Epoch: 675 Loss: 8.558421e-05\n",
      "Epoch: 676 Loss: 0.00010146599\n",
      "Epoch: 677 Loss: 8.3415616e-05\n",
      "Epoch: 678 Loss: 0.00018997489\n",
      "Epoch: 679 Loss: 0.00012619072\n",
      "Epoch: 680 Loss: 7.091352e-05\n",
      "Epoch: 681 Loss: 0.00014246933\n",
      "Epoch: 682 Loss: 0.0001075874\n",
      "Epoch: 683 Loss: 0.000100329555\n",
      "Epoch: 684 Loss: 9.476194e-05\n",
      "Epoch: 685 Loss: 0.00011147477\n",
      "Epoch: 686 Loss: 0.00012583738\n",
      "Epoch: 687 Loss: 7.894155e-05\n",
      "Epoch: 688 Loss: 0.00014487038\n",
      "Epoch: 689 Loss: 0.00014394817\n",
      "Epoch: 690 Loss: 9.636558e-05\n",
      "Epoch: 691 Loss: 9.4605726e-05\n",
      "Epoch: 692 Loss: 6.714327e-05\n",
      "Epoch: 693 Loss: 0.0001428828\n",
      "Epoch: 694 Loss: 0.00013560544\n",
      "Epoch: 695 Loss: 0.00013995796\n",
      "Epoch: 696 Loss: 0.00013871388\n",
      "Epoch: 697 Loss: 8.076924e-05\n",
      "Epoch: 698 Loss: 9.406913e-05\n",
      "Epoch: 699 Loss: 7.111625e-05\n",
      "Epoch: 700 Loss: 0.0001661771\n",
      "Epoch: 701 Loss: 0.00012737402\n",
      "Epoch: 702 Loss: 7.230023e-05\n",
      "Epoch: 703 Loss: 0.0001353566\n",
      "Epoch: 704 Loss: 9.2917566e-05\n",
      "Epoch: 705 Loss: 9.471863e-05\n",
      "Epoch: 706 Loss: 8.935056e-05\n",
      "Epoch: 707 Loss: 0.00012982456\n",
      "Epoch: 708 Loss: 0.000117078125\n",
      "Epoch: 709 Loss: 6.947623e-05\n",
      "Epoch: 710 Loss: 0.00013589027\n",
      "Epoch: 711 Loss: 0.00013166986\n",
      "Epoch: 712 Loss: 9.687896e-05\n",
      "Epoch: 713 Loss: 8.916526e-05\n",
      "Epoch: 714 Loss: 6.927711e-05\n",
      "Epoch: 715 Loss: 0.00013160422\n",
      "Epoch: 716 Loss: 0.00010339169\n",
      "Epoch: 717 Loss: 0.00013367705\n",
      "Epoch: 718 Loss: 0.00013107002\n",
      "Epoch: 719 Loss: 7.7198725e-05\n",
      "Epoch: 720 Loss: 8.833604e-05\n",
      "Epoch: 721 Loss: 6.2065454e-05\n",
      "Epoch: 722 Loss: 0.0001475199\n",
      "Epoch: 723 Loss: 0.00012851102\n",
      "Epoch: 724 Loss: 7.821832e-05\n",
      "Epoch: 725 Loss: 0.00012961538\n",
      "Epoch: 726 Loss: 8.047361e-05\n",
      "Epoch: 727 Loss: 9.082131e-05\n",
      "Epoch: 728 Loss: 8.413771e-05\n",
      "Epoch: 729 Loss: 0.00015909503\n",
      "Epoch: 730 Loss: 0.00010924379\n",
      "Epoch: 731 Loss: 6.30236e-05\n",
      "Epoch: 732 Loss: 0.00012784905\n",
      "Epoch: 733 Loss: 0.000108489905\n",
      "Epoch: 734 Loss: 9.233075e-05\n",
      "Epoch: 735 Loss: 8.4613115e-05\n",
      "Epoch: 736 Loss: 7.372922e-05\n",
      "Epoch: 737 Loss: 0.00012091191\n",
      "Epoch: 738 Loss: 8.247813e-05\n",
      "Epoch: 739 Loss: 0.00012861901\n",
      "Epoch: 740 Loss: 0.00012538579\n",
      "Epoch: 741 Loss: 7.63965e-05\n",
      "Epoch: 742 Loss: 8.411325e-05\n",
      "Epoch: 743 Loss: 5.713337e-05\n",
      "Epoch: 744 Loss: 0.000135121\n",
      "Epoch: 745 Loss: 0.00012408351\n",
      "Epoch: 746 Loss: 9.003937e-05\n",
      "Epoch: 747 Loss: 0.00012349023\n",
      "Epoch: 748 Loss: 7.236831e-05\n",
      "Epoch: 749 Loss: 8.609347e-05\n",
      "Epoch: 750 Loss: 7.492054e-05\n",
      "Epoch: 751 Loss: 0.00016636947\n",
      "Epoch: 752 Loss: 0.00010435386\n",
      "Epoch: 753 Loss: 5.8218808e-05\n",
      "Epoch: 754 Loss: 0.00012071434\n",
      "Epoch: 755 Loss: 9.350839e-05\n",
      "Epoch: 756 Loss: 8.5541884e-05\n",
      "Epoch: 757 Loss: 8.0655096e-05\n",
      "Epoch: 758 Loss: 8.204916e-05\n",
      "Epoch: 759 Loss: 0.000109250184\n",
      "Epoch: 760 Loss: 6.943225e-05\n",
      "Epoch: 761 Loss: 0.00012306076\n",
      "Epoch: 762 Loss: 0.000120976656\n",
      "Epoch: 763 Loss: 7.7989476e-05\n",
      "Epoch: 764 Loss: 8.014651e-05\n",
      "Epoch: 765 Loss: 5.4892676e-05\n",
      "Epoch: 766 Loss: 0.00012119762\n",
      "Epoch: 767 Loss: 0.000119256525\n",
      "Epoch: 768 Loss: 0.0001125426\n",
      "Epoch: 769 Loss: 0.00011849952\n",
      "Epoch: 770 Loss: 6.75277e-05\n",
      "Epoch: 771 Loss: 8.0601385e-05\n",
      "Epoch: 772 Loss: 6.3432686e-05\n",
      "Epoch: 773 Loss: 0.00014697491\n",
      "Epoch: 774 Loss: 0.00010202238\n",
      "Epoch: 775 Loss: 5.7617704e-05\n",
      "Epoch: 776 Loss: 0.0001169566\n",
      "Epoch: 777 Loss: 8.3214254e-05\n",
      "Epoch: 778 Loss: 8.015223e-05\n",
      "Epoch: 779 Loss: 7.5433236e-05\n",
      "Epoch: 780 Loss: 9.622717e-05\n",
      "Epoch: 781 Loss: 9.990639e-05\n",
      "Epoch: 782 Loss: 6.105073e-05\n",
      "Epoch: 783 Loss: 0.00011868159\n",
      "Epoch: 784 Loss: 0.0001164705\n",
      "Epoch: 785 Loss: 7.977895e-05\n",
      "Epoch: 786 Loss: 7.555173e-05\n",
      "Epoch: 787 Loss: 5.5453773e-05\n",
      "Epoch: 788 Loss: 0.00011260376\n",
      "Epoch: 789 Loss: 9.948913e-05\n",
      "Epoch: 790 Loss: 0.00011524585\n",
      "Epoch: 791 Loss: 0.000111498455\n",
      "Epoch: 792 Loss: 6.4730455e-05\n",
      "Epoch: 793 Loss: 7.503948e-05\n",
      "Epoch: 794 Loss: 5.519376e-05\n",
      "Epoch: 795 Loss: 0.00012839475\n",
      "Epoch: 796 Loss: 0.00010679179\n",
      "Epoch: 797 Loss: 6.0946535e-05\n",
      "Epoch: 798 Loss: 0.00011050661\n",
      "Epoch: 799 Loss: 7.177829e-05\n",
      "Epoch: 800 Loss: 7.6325676e-05\n",
      "Epoch: 801 Loss: 7.1399525e-05\n",
      "Epoch: 802 Loss: 0.000116918076\n",
      "Epoch: 803 Loss: 9.4192364e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 804 Loss: 5.4290012e-05\n",
      "Epoch: 805 Loss: 0.00011126937\n",
      "Epoch: 806 Loss: 0.00010039801\n",
      "Epoch: 807 Loss: 7.9192614e-05\n",
      "Epoch: 808 Loss: 7.185477e-05\n",
      "Epoch: 809 Loss: 5.8374866e-05\n",
      "Epoch: 810 Loss: 0.00010545766\n",
      "Epoch: 811 Loss: 7.746503e-05\n",
      "Epoch: 812 Loss: 0.00011081501\n",
      "Epoch: 813 Loss: 0.000105919156\n",
      "Epoch: 814 Loss: 6.289917e-05\n",
      "Epoch: 815 Loss: 7.129316e-05\n",
      "Epoch: 816 Loss: 4.9010243e-05\n",
      "Epoch: 817 Loss: 0.00011819103\n",
      "Epoch: 818 Loss: 0.00010494532\n",
      "Epoch: 819 Loss: 6.797177e-05\n",
      "Epoch: 820 Loss: 0.00010509223\n",
      "Epoch: 821 Loss: 6.266612e-05\n",
      "Epoch: 822 Loss: 7.3912255e-05\n",
      "Epoch: 823 Loss: 6.8347144e-05\n",
      "Epoch: 824 Loss: 0.00014434088\n",
      "Epoch: 825 Loss: 8.8465116e-05\n",
      "Epoch: 826 Loss: 5.0052753e-05\n",
      "Epoch: 827 Loss: 0.00010511539\n",
      "Epoch: 828 Loss: 8.415709e-05\n",
      "Epoch: 829 Loss: 7.462349e-05\n",
      "Epoch: 830 Loss: 6.930943e-05\n",
      "Epoch: 831 Loss: 6.2683706e-05\n",
      "Epoch: 832 Loss: 9.644278e-05\n",
      "Epoch: 833 Loss: 6.320613e-05\n",
      "Epoch: 834 Loss: 0.00010706398\n",
      "Epoch: 835 Loss: 0.0001022428\n",
      "Epoch: 836 Loss: 6.3457264e-05\n",
      "Epoch: 837 Loss: 6.871703e-05\n",
      "Epoch: 838 Loss: 4.6535057e-05\n",
      "Epoch: 839 Loss: 0.00010675178\n",
      "Epoch: 840 Loss: 0.000101660895\n",
      "Epoch: 841 Loss: 8.093824e-05\n",
      "Epoch: 842 Loss: 0.00010044763\n",
      "Epoch: 843 Loss: 5.7725516e-05\n",
      "Epoch: 844 Loss: 6.966812e-05\n",
      "Epoch: 845 Loss: 5.8271162e-05\n",
      "Epoch: 846 Loss: 0.00013241573\n",
      "Epoch: 847 Loss: 8.5706895e-05\n",
      "Epoch: 848 Loss: 4.7736186e-05\n",
      "Epoch: 849 Loss: 9.972812e-05\n",
      "Epoch: 850 Loss: 7.432455e-05\n",
      "Epoch: 851 Loss: 6.9064525e-05\n",
      "Epoch: 852 Loss: 6.5540306e-05\n",
      "Epoch: 853 Loss: 7.280692e-05\n",
      "Epoch: 854 Loss: 8.7413086e-05\n",
      "Epoch: 855 Loss: 5.4404092e-05\n",
      "Epoch: 856 Loss: 0.000102011225\n",
      "Epoch: 857 Loss: 9.888827e-05\n",
      "Epoch: 858 Loss: 6.5534e-05\n",
      "Epoch: 859 Loss: 6.547246e-05\n",
      "Epoch: 860 Loss: 4.5641827e-05\n",
      "Epoch: 861 Loss: 9.9022305e-05\n",
      "Epoch: 862 Loss: 9.740564e-05\n",
      "Epoch: 863 Loss: 9.7606484e-05\n",
      "Epoch: 864 Loss: 9.594909e-05\n",
      "Epoch: 865 Loss: 5.5071614e-05\n",
      "Epoch: 866 Loss: 6.54141e-05\n",
      "Epoch: 867 Loss: 4.9831302e-05\n",
      "Epoch: 868 Loss: 0.00011723097\n",
      "Epoch: 869 Loss: 8.720895e-05\n",
      "Epoch: 870 Loss: 4.8559155e-05\n",
      "Epoch: 871 Loss: 9.508315e-05\n",
      "Epoch: 872 Loss: 6.4832035e-05\n",
      "Epoch: 873 Loss: 6.606654e-05\n",
      "Epoch: 874 Loss: 6.2163475e-05\n",
      "Epoch: 875 Loss: 8.656615e-05\n",
      "Epoch: 876 Loss: 8.179406e-05\n",
      "Epoch: 877 Loss: 4.7985137e-05\n",
      "Epoch: 878 Loss: 9.64153e-05\n",
      "Epoch: 879 Loss: 9.461784e-05\n",
      "Epoch: 880 Loss: 6.810559e-05\n",
      "Epoch: 881 Loss: 6.2249834e-05\n",
      "Epoch: 882 Loss: 4.7087062e-05\n",
      "Epoch: 883 Loss: 9.206376e-05\n",
      "Epoch: 884 Loss: 7.4921e-05\n",
      "Epoch: 885 Loss: 9.3464536e-05\n",
      "Epoch: 886 Loss: 9.1111e-05\n",
      "Epoch: 887 Loss: 5.322671e-05\n",
      "Epoch: 888 Loss: 6.1697414e-05\n",
      "Epoch: 889 Loss: 4.3594908e-05\n",
      "Epoch: 890 Loss: 0.00010385668\n",
      "Epoch: 891 Loss: 8.961106e-05\n",
      "Epoch: 892 Loss: 5.274074e-05\n",
      "Epoch: 893 Loss: 9.046222e-05\n",
      "Epoch: 894 Loss: 5.6583733e-05\n",
      "Epoch: 895 Loss: 6.382716e-05\n",
      "Epoch: 896 Loss: 5.8663805e-05\n",
      "Epoch: 897 Loss: 0.00010655364\n",
      "Epoch: 898 Loss: 7.681397e-05\n",
      "Epoch: 899 Loss: 4.3625532e-05\n",
      "Epoch: 900 Loss: 9.11721e-05\n",
      "Epoch: 901 Loss: 7.856419e-05\n",
      "Epoch: 902 Loss: 6.535444e-05\n",
      "Epoch: 903 Loss: 5.9449856e-05\n",
      "Epoch: 904 Loss: 5.034754e-05\n",
      "Epoch: 905 Loss: 8.626083e-05\n",
      "Epoch: 906 Loss: 5.930677e-05\n",
      "Epoch: 907 Loss: 9.0654554e-05\n",
      "Epoch: 908 Loss: 8.7685745e-05\n",
      "Epoch: 909 Loss: 5.269682e-05\n",
      "Epoch: 910 Loss: 5.9173533e-05\n",
      "Epoch: 911 Loss: 4.0077306e-05\n",
      "Epoch: 912 Loss: 9.5598625e-05\n",
      "Epoch: 913 Loss: 8.773181e-05\n",
      "Epoch: 914 Loss: 6.087859e-05\n",
      "Epoch: 915 Loss: 8.6802516e-05\n",
      "Epoch: 916 Loss: 5.0512645e-05\n",
      "Epoch: 917 Loss: 6.1059895e-05\n",
      "Epoch: 918 Loss: 5.4153355e-05\n",
      "Epoch: 919 Loss: 0.00011976313\n",
      "Epoch: 920 Loss: 7.341784e-05\n",
      "Epoch: 921 Loss: 4.074559e-05\n",
      "Epoch: 922 Loss: 8.666121e-05\n",
      "Epoch: 923 Loss: 6.7883324e-05\n",
      "Epoch: 924 Loss: 6.1022605e-05\n",
      "Epoch: 925 Loss: 5.731496e-05\n",
      "Epoch: 926 Loss: 5.5733402e-05\n",
      "Epoch: 927 Loss: 7.843212e-05\n",
      "Epoch: 928 Loss: 4.9603448e-05\n",
      "Epoch: 929 Loss: 8.751599e-05\n",
      "Epoch: 930 Loss: 8.514996e-05\n",
      "Epoch: 931 Loss: 5.4073644e-05\n",
      "Epoch: 932 Loss: 5.6870784e-05\n",
      "Epoch: 933 Loss: 3.8695904e-05\n",
      "Epoch: 934 Loss: 8.784045e-05\n",
      "Epoch: 935 Loss: 8.562697e-05\n",
      "Epoch: 936 Loss: 7.454371e-05\n",
      "Epoch: 937 Loss: 8.331631e-05\n",
      "Epoch: 938 Loss: 4.755067e-05\n",
      "Epoch: 939 Loss: 5.746949e-05\n",
      "Epoch: 940 Loss: 4.592109e-05\n",
      "Epoch: 941 Loss: 0.0001074607\n",
      "Epoch: 942 Loss: 7.257823e-05\n",
      "Epoch: 943 Loss: 4.003613e-05\n",
      "Epoch: 944 Loss: 8.2846855e-05\n",
      "Epoch: 945 Loss: 5.9402977e-05\n",
      "Epoch: 946 Loss: 5.750111e-05\n",
      "Epoch: 947 Loss: 5.3890246e-05\n",
      "Epoch: 948 Loss: 6.667079e-05\n",
      "Epoch: 949 Loss: 7.177563e-05\n",
      "Epoch: 950 Loss: 4.343782e-05\n",
      "Epoch: 951 Loss: 8.358538e-05\n",
      "Epoch: 952 Loss: 8.283563e-05\n",
      "Epoch: 953 Loss: 5.6580484e-05\n",
      "Epoch: 954 Loss: 5.432188e-05\n",
      "Epoch: 955 Loss: 3.888003e-05\n",
      "Epoch: 956 Loss: 8.1190425e-05\n",
      "Epoch: 957 Loss: 7.43844e-05\n",
      "Epoch: 958 Loss: 8.1336606e-05\n",
      "Epoch: 959 Loss: 7.985046e-05\n",
      "Epoch: 960 Loss: 4.553788e-05\n",
      "Epoch: 961 Loss: 5.416801e-05\n",
      "Epoch: 962 Loss: 3.981471e-05\n",
      "Epoch: 963 Loss: 9.430186e-05\n",
      "Epoch: 964 Loss: 7.570379e-05\n",
      "Epoch: 965 Loss: 4.2143824e-05\n",
      "Epoch: 966 Loss: 7.939799e-05\n",
      "Epoch: 967 Loss: 5.1613806e-05\n",
      "Epoch: 968 Loss: 5.508464e-05\n",
      "Epoch: 969 Loss: 5.1620522e-05\n",
      "Epoch: 970 Loss: 8.030148e-05\n",
      "Epoch: 971 Loss: 6.7751556e-05\n",
      "Epoch: 972 Loss: 3.8839393e-05\n",
      "Epoch: 973 Loss: 7.939939e-05\n",
      "Epoch: 974 Loss: 7.3937634e-05\n",
      "Epoch: 975 Loss: 5.6690275e-05\n",
      "Epoch: 976 Loss: 5.1957708e-05\n",
      "Epoch: 977 Loss: 4.1021874e-05\n",
      "Epoch: 978 Loss: 7.638988e-05\n",
      "Epoch: 979 Loss: 5.7180245e-05\n",
      "Epoch: 980 Loss: 7.883339e-05\n",
      "Epoch: 981 Loss: 7.664087e-05\n",
      "Epoch: 982 Loss: 4.4544493e-05\n",
      "Epoch: 983 Loss: 5.151664e-05\n",
      "Epoch: 984 Loss: 3.530906e-05\n",
      "Epoch: 985 Loss: 8.5516855e-05\n",
      "Epoch: 986 Loss: 7.625821e-05\n",
      "Epoch: 987 Loss: 4.7087386e-05\n",
      "Epoch: 988 Loss: 7.6179655e-05\n",
      "Epoch: 989 Loss: 4.524307e-05\n",
      "Epoch: 990 Loss: 5.3639098e-05\n",
      "Epoch: 991 Loss: 4.9193626e-05\n",
      "Epoch: 992 Loss: 0.00010063951\n",
      "Epoch: 993 Loss: 6.3484746e-05\n",
      "Epoch: 994 Loss: 3.595287e-05\n",
      "Epoch: 995 Loss: 7.5517404e-05\n",
      "Epoch: 996 Loss: 6.286575e-05\n",
      "Epoch: 997 Loss: 5.4405984e-05\n",
      "Epoch: 998 Loss: 4.9890554e-05\n",
      "Epoch: 999 Loss: 4.4332308e-05\n",
      "Epoch: 1000 Loss: 7.075426e-05\n"
     ]
    }
   ],
   "source": [
    "# Setting Batch with Training\n",
    "batch_size = 1261\n",
    "epoch = 1000\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('tf_board', sess.graph)\n",
    "    for i in range(epoch):\n",
    "        batch_data, batch_label = batch(trainlist, batch_size)     \n",
    "        _, loss, summary = sess.run([train_step, Loss, merged], feed_dict = {X: batch_data, Y: batch_label})\n",
    "        print(\"Epoch:\",i+1,\"Loss:\",loss)\n",
    "        if i % 10 == 0:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)\n",
    "        elif i+1 == epoch:\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            saver.save(sess, 'logs/model.ckpt', global_step = i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs\\model.ckpt-1000\n",
      "[-0.00776708  1.0595394  -1.8102638 ] 0\n",
      "[-0.08547989  1.3054487  -1.6776209 ] 0\n",
      "[-0.03934199  1.2908739  -1.863743  ] 0\n",
      "[ 0.280813   1.1099066 -1.6594608] 0\n",
      "[ 0.12458479  0.5998282  -1.8284099 ] 0\n",
      "[ 0.36861125  0.9803136  -1.4008007 ] 0\n",
      "[ 0.49296266  0.8132829  -1.3991555 ] 0\n",
      "[ 0.22975308  0.6291299  -1.7825828 ] 0\n",
      "[-0.44606525  0.21892989  1.2427061 ] 1\n",
      "[-1.7054174  1.6543214  1.8001354] 1\n",
      "[-0.745101    0.63360816  1.7450112 ] 1\n",
      "[ 0.7857095   0.6305351  -0.03069509] 1\n",
      "Accuracy: 0.9755102040816327\n"
     ]
    }
   ],
   "source": [
    "# Print an Accuracy\n",
    "acc = 0\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint('logs')\n",
    "    if checkpoint:\n",
    "        saver.restore(sess, checkpoint)\n",
    "    for i in range(len(testlist)):\n",
    "        batch_data, batch_label = batch(testlist, 1)\n",
    "        logit = sess.run(output, feed_dict = {X:batch_data})\n",
    "        if np.argmax(logit[0]) == batch_label[0]:\n",
    "            acc += 1\n",
    "        else:\n",
    "            print(logit[0], batch_label[0])\n",
    "            \n",
    "    accuracy = acc/len(testlist)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an Answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
